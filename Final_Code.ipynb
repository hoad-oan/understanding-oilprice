{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tIIGKQKZc5_"
      },
      "source": [
        "**Import Package**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAoG9iPtZgn4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F7Sgbo_4xHy"
      },
      "source": [
        "## Collecting tweets and news exerpts: Sentiment and Emotion Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_Z6oHbb46HP"
      },
      "outputs": [],
      "source": [
        "# conducting sentiment analysis\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# downloading lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# instantiating model\n",
        "sia = SentimentIntensityAnalyzer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOY2NJCX488Q"
      },
      "outputs": [],
      "source": [
        "def sentiment_analysis(tweet_text): # need to add emotional score into the list\n",
        "    score = sia.polarity_scores(tweet_text)[\"compound\"]\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdaOCvsO4-i3"
      },
      "outputs": [],
      "source": [
        "# conducting emotional analysis\n",
        "\n",
        "#import requests\n",
        "import json\n",
        "from ibm_watson import NaturalLanguageUnderstandingV1\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "from ibm_watson.natural_language_understanding_v1 import Features, EmotionOptions, SentimentOptions\n",
        "\n",
        "base_url = 'insert credentials'\n",
        "api_key = 'insert credentials'\n",
        "\n",
        "authenticator = IAMAuthenticator(api_key)\n",
        "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
        "    version='2022-04-07',\n",
        "    authenticator=authenticator)\n",
        "\n",
        "natural_language_understanding.set_service_url(base_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xiove-zy5Myb"
      },
      "outputs": [],
      "source": [
        "def emotional_analysis(text_content):\n",
        "    response = natural_language_understanding.analyze(text=text_content, features=Features(emotion=EmotionOptions(document=True)), language='en').get_result()\n",
        "    res = []\n",
        "    # output = json.dumps(response, indent=2)\n",
        "    emotions = response['emotion']['document']['emotion']\n",
        "    for key, value in emotions.items():\n",
        "        if key == 'sadness':\n",
        "            sadness_score = value\n",
        "        if key == 'joy':\n",
        "            joy_score = value\n",
        "        if key == 'fear':\n",
        "            fear_score = value\n",
        "        if key == 'disgust':\n",
        "            disgust_score = value\n",
        "        if key == 'anger':\n",
        "            anger_score = value\n",
        "    res = [sadness_score, joy_score, fear_score, disgust_score, anger_score]\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aObR1prS5O_m"
      },
      "outputs": [],
      "source": [
        "# connecting to twitter\n",
        "import tweepy, pandas\n",
        "\n",
        "# creating the 4 keys \n",
        "consumer_key = 'insert credentials'\n",
        "consumer_secret = 'insert credentials'\n",
        "access_token = 'insert credentials'\n",
        "access_token_secret = 'insert credentials'\n",
        "\n",
        "# authenticating\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfpHQ3dJ5ags"
      },
      "outputs": [],
      "source": [
        "# import the required modules\n",
        "from time import sleep\n",
        "\n",
        "# define relevant variables\n",
        "# since there were limitations in how many tweets I can collect at once, I only collect up to 1000 tweets everyday, eliminating the retweets\n",
        "seen = set() # this set collect tweet id to verify if a tweet was collected\n",
        "tweet_df = pandas.DataFrame(columns = [\"Tweet's Text\", \"Sentiment Score\", \"Sadness Score\", \"Joy Score\", \"Fear Score\", \\\n",
        "    \"Disgust Score\", \"Anger Score\", \"Hashtags\", \"Created At\"])\n",
        "\n",
        "while True:\n",
        "    # retrieve five tweets\n",
        "    for tweet in tweepy.Cursor(api.search_tweets, q=\"oil price OR gas price\", lang=\"en\", since_id = \"2022-05-04\", until=\"2022-05-04\").items(1000):\n",
        "        # verify whether tweet was not collected previously or if the tweet is a retweet\n",
        "        if (tweet.id not in seen) and ('RT @' not in tweet.text):\n",
        "            seen.add(tweet.id)\n",
        "            # calculate the sentiment score associated with tweet\n",
        "            hashtag = tweet.entities[\"hashtags\"]\n",
        "            sentiment_score = sentiment_analysis(tweet.text)\n",
        "            sadness_score, joy_score, fear_score, disgust_score, anger_score = emotional_analysis(tweet.text)\n",
        "            # add to current df\n",
        "            ls = [tweet.text, sentiment_score, sadness_score, joy_score, fear_score, disgust_score, anger_score, [hashtag if hashtag else None], tweet.created_at]\n",
        "            tweet_df.loc[len(tweet_df)] = ls\n",
        "\n",
        "    # # print current cumulative_score\n",
        "    print(tweet_df)\n",
        "    # sleeping for 10 seconds to avoid too many Twitter queries\n",
        "    sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7wxje0v6PWP"
      },
      "outputs": [],
      "source": [
        "# export tweet dataframe daily\n",
        "tweet_df.to_csv(\"tweet_df_0504.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZIl3GDo6YIE"
      },
      "outputs": [],
      "source": [
        "# pull data from news article using newscatcherapi\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import pandas as pd\n",
        "\n",
        "# Put your API key to headers in order to be authorized to perform a call\n",
        "headers = {'x-api-key': 'insert credentials'}\n",
        "url = \"https://api.newscatcherapi.com/v2/search\"\n",
        "start_date = datetime.date(2022, 4, 1)\n",
        "end_date = datetime.date(2022, 4, 29)\n",
        "delta = datetime.timedelta(days=1)\n",
        "\n",
        "while start_date <= end_date:\n",
        "\n",
        "    # Define your desired parameters\n",
        "    params = {\n",
        "        'q': 'oil price OR gas price',\n",
        "        'lang': 'en',\n",
        "        'to_rank': 10000,\n",
        "        'page_size': 100,\n",
        "        'page': 1,\n",
        "        'from': start_date,\n",
        "        'to': start_date\n",
        "        }\n",
        "\n",
        "    # Make a simple call with both headers and params\n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "    # Encode received results\n",
        "    article_results = json.loads(response.text.encode())\n",
        "    if response.status_code == 200:\n",
        "        print('Done')\n",
        "    else:\n",
        "        print(article_results)\n",
        "        print('ERROR: API call failed.')\n",
        "    # add to current data frame\n",
        "    news_data = pd.DataFrame(article_results['articles'])\n",
        "    all_news_data = all_news_data.append(news_data)\n",
        "\n",
        "    start_date = start_date + delta\n",
        "    time.sleep(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjN5ZYAcdwp8"
      },
      "outputs": [],
      "source": [
        "# Using lambda function to apply sentiment score and emotional score. All NAs were removed in csv files and imported back to python\n",
        "all_news_data['sentiment_score'] = all_news_data.apply(lambda row : sentiment_analysis(row[8]), axis=1)\n",
        "all_news_data['emotional_score'] = all_news_data.apply(lambda row : emotional_analysis(row[8]), axis=1)\n",
        "all_news_data[['sadness_score', 'joy_score', 'fear_score', 'disgust_score', 'anger_score']] = pd.DataFrame(all_news_data.emotional_score.tolist(), index= all_news_data.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHdlFUcxGtH4"
      },
      "outputs": [],
      "source": [
        "# exporting news data \n",
        "all_news_data.to_csv('news_article_april.csv')\n",
        "## note: since there's a time limitation in how much data can be collected through this api (30 days into the past), news data for april and may are saved in different files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE7PA_74eHv8"
      },
      "source": [
        "**Granger Test on Tweets and News Articles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVV0oWhMeG_7"
      },
      "outputs": [],
      "source": [
        "tweets_sum_scores = pd.read_csv('tweets_sum_scores_noNA.csv')\n",
        "sadness_score = tweets_sum_scores['sadness_score'].to_list()\n",
        "oil_price = tweets_sum_scores['WTIOIL'].to_list()\n",
        "\n",
        "# news_article_sum_scores = pd.read_csv('news_article_sum_scores_noNA.csv') # use this line to read in news articles data\n",
        "\n",
        "sadness_score = tweets_sum_scores['sadness_score'].to_list() # replace the tweets_sum_scores with the news_article_sum_scores to run test on news data\n",
        "oil_price = tweets_sum_scores['WTIOIL'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWhivGjweP7u"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 2 plots were created to showcase different time series vs oil time series data as sentiment and emotions scores are much smaller\n",
        "# first time series\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(sadness_score, color='b')\n",
        "\n",
        "plt.legend(['sadness_score'], fontsize=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ4p-zTyej_-"
      },
      "outputs": [],
      "source": [
        "# second time series\n",
        "plt.figure(figsize=(10,4))\n",
        "# plt.plot(fear_score, color='b')\n",
        "plt.plot(oil_price, color='r')\n",
        "\n",
        "plt.legend(['oil_price'], fontsize=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5uv2r3aeykQ"
      },
      "outputs": [],
      "source": [
        "# perform Granger on dataset\n",
        "ts_df = pd.DataFrame(columns=['oil_price', 'sadness_score'], data=zip(oil_price, sadness_score))\n",
        "ts_df = ts_df.iloc[:-2] # eliminate the last 2 row, one without oil price data and a blank row that was imported\n",
        "gc_res = grangercausalitytests(ts_df, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltERgF6u52-R"
      },
      "source": [
        "## Analyzing the relationship between stock market and oil price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aICy2NAH50M6"
      },
      "outputs": [],
      "source": [
        "# importing stock data from yfinance\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "df_energy = yf.download('^GSPE', '2018-01-06', interval = '1wk')\n",
        "df_healthcare = yf.download('^SP500-35', '2018-01-06', interval = '1wk')\n",
        "df_realestate = yf.download('^SP500-60', '2018-01-06', interval = '1wk')\n",
        "df_material = yf.download('^SP500-15', '2018-01-06', interval = '1wk')\n",
        "df_industrials = yf.download('^SP500-20', '2018-01-06', interval = '1wk')\n",
        "df_financials = yf.download('^SP500-40', '2018-01-06', interval = '1wk')\n",
        "df_utilities = yf.download('^SP500-55', '2018-01-06', interval = '1wk')\n",
        "df_info_technology = yf.download('^SP500-45', '2018-01-06', interval = '1wk')\n",
        "df_communication = yf.download('^SP500-50', '2018-01-06', interval = '1wk')\n",
        "df_consumer_discretionary = yf.download('^SP500-25', '2018-01-06', interval = '1wk')\n",
        "df_consumer_staples = yf.download('^SP500-30', '2018-01-06', interval = '1wk')\n",
        "oil_price = pd.read_csv('WCOILWTICO.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmRBnGig5-6_"
      },
      "outputs": [],
      "source": [
        "# subsetting the dataframes; will only be looking at the Adj Close price\n",
        "df_energy = df_energy[['Adj Close']]\n",
        "df_healthcare = df_healthcare[['Adj Close']]\n",
        "df_realestate = df_realestate[['Adj Close']]\n",
        "df_material = df_material[['Adj Close']]\n",
        "df_industrials = df_industrials[['Adj Close']]\n",
        "df_financials = df_financials[['Adj Close']]\n",
        "df_utilities = df_utilities[['Adj Close']]\n",
        "df_info_technology = df_info_technology[['Adj Close']]\n",
        "df_communication = df_communication[['Adj Close']]\n",
        "df_consumer_discretionary = df_consumer_discretionary[['Adj Close']]\n",
        "df_consumer_staples = df_consumer_staples[['Adj Close']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB1Jtnus6EEQ"
      },
      "outputs": [],
      "source": [
        "# rename column for the stocks\n",
        "df_energy = df_energy.rename(columns={'Adj Close': 'Energy'})\n",
        "df_healthcare = df_healthcare.rename(columns={'Adj Close': 'Healthcare'})\n",
        "df_realestate = df_realestate.rename(columns={'Adj Close': 'Real Estate'})\n",
        "df_material = df_material.rename(columns={'Adj Close': 'Material'})\n",
        "df_industrials = df_industrials.rename(columns={'Adj Close': 'Industrials'})\n",
        "df_financials = df_financials.rename(columns={'Adj Close': 'Financials'})\n",
        "df_utilities = df_utilities.rename(columns={'Adj Close': 'Utilities'})\n",
        "df_info_technology = df_info_technology.rename(columns={'Adj Close': 'Info Tech'})\n",
        "df_communication = df_communication.rename(columns={'Adj Close': 'Communication'})\n",
        "df_consumer_discretionary = df_consumer_discretionary.rename(columns={'Adj Close': 'Consumer Discretionary'})\n",
        "df_consumer_staples = df_consumer_staples.rename(columns={'Adj Close': 'Consumer Staples'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jjKCDZU6GBA"
      },
      "outputs": [],
      "source": [
        "# merge stock df\n",
        "merged_stock_df = pd.merge(df_energy, df_healthcare, left_index=True, right_index=True, how='left')\n",
        "merged_stock_df1 = pd.merge(df_realestate, df_material, left_index=True, right_index=True, how='left')\n",
        "merged_stock_df2 = pd.merge(df_industrials, df_financials, left_index=True, right_index=True, how='left')\n",
        "merged_stock_df3 = pd.merge(df_utilities, df_info_technology, left_index=True, right_index=True, how='left')\n",
        "merged_stock_df4 = pd.merge(df_communication, df_consumer_discretionary, left_index=True, right_index=True, how='left')\n",
        "# second merge\n",
        "merged_stock_df = pd.merge(merged_stock_df, merged_stock_df1, left_index=True, right_index=True, how='left')\n",
        "merged_stock_df2 = pd.merge(merged_stock_df2, merged_stock_df3, left_index=True, right_index=True, how='left')\n",
        "merged_stock_df4 = pd.merge(merged_stock_df4, df_consumer_staples, left_index=True, right_index=True, how='left')\n",
        "# third merge\n",
        "merged_stock_df = pd.merge(merged_stock_df, merged_stock_df2, left_index=True, right_index=True, how='left')\n",
        "merged_stock_df = pd.merge(merged_stock_df, merged_stock_df4, left_index=True, right_index=True, how='left')\n",
        "# export the dataframe\n",
        "merged_stock_df.to_csv(\"merged_stock_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2X63ISM6MwE"
      },
      "outputs": [],
      "source": [
        "# create correlation matrix between sector index and oil price\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stock_df = pd.read_csv('merged_stock_df.csv')\n",
        "# stock_df = stock_df.iloc[:-123] # run this line of code to get data from 2018-2019 only\n",
        "\n",
        "hm = sns.heatmap(stock_df.corr(), annot = True)\n",
        "\n",
        "hm.set(title = \"Correlation matrix of Sector Index with Oil Price\\n\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwF0kcJcfnP7"
      },
      "outputs": [],
      "source": [
        "# perform Granger test on the ones with high correlations, except Energy \n",
        "oil_price = stock_df['WCOILWTICO'].to_list()\n",
        "financials = stock_df['Financials'].to_list()\n",
        "\n",
        "# run these codes to get data for Real Estate sector\n",
        "# real_estate = stock_df['Real Estate'].to_list()\n",
        "# real_estate = [x for x in real_estate if pd.isnull(x) == False]\n",
        "# run these codes to get data for Utilities sector\n",
        "# real_estate = stock_df['Real Estate'].to_list()\n",
        "\n",
        "# run these codes to perform granger test for stock related indices\n",
        "ts2_df = pd.DataFrame(columns=['oil_price', 'financials'], data=zip(oil_price, financials))\n",
        "ts2_df = ts2_df.iloc[:-2]\n",
        "gc_res2 = grangercausalitytests(ts2_df, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clJWhozSgD9Y"
      },
      "source": [
        "**Fear Index vs Oil Price**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lMwUwb5gGjM"
      },
      "outputs": [],
      "source": [
        "# gathering data for VIX\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "df_vix = yf.download('^VIX', '2018-01-06', interval = '1wk')\n",
        "df_vix = df_vix[['Adj Close']]\n",
        "df_vix = df_vix.rename(columns={'Adj Close': '^VIX'})\n",
        "df_vix.to_csv(\"df_vix.csv\")\n",
        "# merge df_vix with oil price on excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdjEZn6YgLDX"
      },
      "outputs": [],
      "source": [
        "#see causality between vix causing oilprice from 2018-2022\n",
        "df_vix = pd.read_csv('df_vix.csv')\n",
        "vix_score = df_vix['^VIX'].to_list()\n",
        "oil_price = df_vix['WCOILWTICO'].to_list()\n",
        "\n",
        "#see causality between vix causing oilprice from 2018-2019\n",
        "# df_vix2 = pd.read_csv('df_vix2.csv')\n",
        "# vix_score = df_vix2['^VIX'].to_list()\n",
        "# oil_price = df_vix2['WCOILWTICO'].to_list()\n",
        "\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# first time series, news sentiments\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(vix_score, color='b')\n",
        "plt.plot(oil_price, color='r')\n",
        "\n",
        "plt.legend(['vix_index', 'oil_price'], fontsize=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFWa2JtTgVS-"
      },
      "outputs": [],
      "source": [
        "ts1_df = pd.DataFrame(columns=['oil_price', 'vix_score'], data=zip(oil_price, vix_score))\n",
        "ts1_df = ts1_df.iloc[:-2]\n",
        "gc_res1 = grangercausalitytests(ts1_df, 3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final Code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
